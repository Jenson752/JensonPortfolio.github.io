{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c114c21d",
   "metadata": {},
   "source": [
    "# TXSA Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d70391",
   "metadata": {},
   "source": [
    "#### ~ Helper Function ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde402a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_functions = [\n",
    "    'DetectStopwords(token)',\n",
    "    'DetectPunctuations(token)'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547f04a",
   "metadata": {},
   "source": [
    "##### - detecting stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4308a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Justin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "hasn't\n",
      "in\n",
      "some\n",
      "his\n",
      "very\n",
      "ve\n",
      "ll\n",
      "the\n",
      "wouldn't\n",
      "is\n",
      "by\n",
      "are\n",
      "off\n",
      "wasn't\n",
      "y\n",
      "their\n",
      "am\n",
      "you've\n",
      "those\n",
      "couldn't\n",
      "me\n",
      "should've\n",
      "yourself\n",
      "didn't\n",
      "will\n",
      "ma\n",
      "through\n",
      "s\n",
      "ain\n",
      "we\n",
      "d\n",
      "yourselves\n",
      "she\n",
      "on\n",
      "against\n",
      "this\n",
      "into\n",
      "after\n",
      "not\n",
      "nor\n",
      "does\n",
      "between\n",
      "wasn\n",
      "too\n",
      "mustn\n",
      "what\n",
      "don't\n",
      "he\n",
      "himself\n",
      "didn\n",
      "above\n",
      "ours\n",
      "if\n",
      "with\n",
      "t\n",
      "you're\n",
      "just\n",
      "why\n",
      "most\n",
      "itself\n",
      "don\n",
      "shan't\n",
      "any\n",
      "now\n",
      "mustn't\n",
      "her\n",
      "shouldn't\n",
      "so\n",
      "mightn't\n",
      "more\n",
      "during\n",
      "who\n",
      "both\n",
      "only\n",
      "own\n",
      "do\n",
      "ourselves\n",
      "re\n",
      "when\n",
      "aren\n",
      "under\n",
      "has\n",
      "same\n",
      "or\n",
      "there\n",
      "being\n",
      "did\n",
      "themselves\n",
      "an\n",
      "our\n",
      "you\n",
      "hers\n",
      "down\n",
      "such\n",
      "having\n",
      "whom\n",
      "my\n",
      "other\n",
      "these\n",
      "here\n",
      "needn\n",
      "up\n",
      "herself\n",
      "few\n",
      "at\n",
      "m\n",
      "him\n",
      "isn't\n",
      "needn't\n",
      "from\n",
      "its\n",
      "of\n",
      "than\n",
      "it's\n",
      "hasn\n",
      "that'll\n",
      "where\n",
      "had\n",
      "before\n",
      "hadn't\n",
      "you'll\n",
      "been\n",
      "then\n",
      "no\n",
      "won't\n",
      "below\n",
      "which\n",
      "weren't\n",
      "about\n",
      "wouldn\n",
      "to\n",
      "all\n",
      "but\n",
      "aren't\n",
      "your\n",
      "because\n",
      "mightn\n",
      "were\n",
      "be\n",
      "you'd\n",
      "have\n",
      "each\n",
      "while\n",
      "weren\n",
      "over\n",
      "shouldn\n",
      "i\n",
      "doing\n",
      "again\n",
      "doesn\n",
      "out\n",
      "for\n",
      "further\n",
      "isn\n",
      "until\n",
      "as\n",
      "myself\n",
      "she's\n",
      "them\n",
      "yours\n",
      "couldn\n",
      "they\n",
      "doesn't\n",
      "that\n",
      "once\n",
      "hadn\n",
      "theirs\n",
      "haven\n",
      "it\n",
      "was\n",
      "how\n",
      "haven't\n",
      "won\n",
      "should\n",
      "and\n",
      "can\n",
      "shan\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# Get list of english stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sw in stop_words:\n",
    "    print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de148df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetectStopwords(token):\n",
    "    if token in stop_words:\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1d37e",
   "metadata": {},
   "source": [
    "#### - detecting punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1535de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acac4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "\n",
    "punctuations = punctuations + '’'\n",
    "\n",
    "def DetectPunctuations(token):\n",
    "    if token in punctuations:\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d54ef",
   "metadata": {},
   "source": [
    "#### - Print Removed Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "405bb0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintRemovedSW(stopwords):\n",
    "    i=0\n",
    "    sw_count_dict = {}\n",
    "    \n",
    "    for sw in stopwords:\n",
    "        if sw in sw_count_dict:\n",
    "            sw_count_dict[sw] += 1\n",
    "            continue\n",
    "        \n",
    "        sw_count_dict[sw] = 1\n",
    "         \n",
    "    print(\"Removed Stopwords:\")\n",
    "    for key,value in sw_count_dict.items():\n",
    "        print(f'{i+1}. {key} ({value})')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4982aa6",
   "metadata": {},
   "source": [
    "#### - Print Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ec2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintDetails(tokens,sws,puncs):\n",
    "    print(f'Total Tokens: {tokens}')\n",
    "    print(f'Total Stop Words: {sws}')\n",
    "    print(f'Total Punctuations: {puncs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffd0c3",
   "metadata": {},
   "source": [
    "## Q1. Form Tokenization and Filter stop words & punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f36c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.']\n",
      "Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.\n"
     ]
    }
   ],
   "source": [
    "# Read Data_1.txt\n",
    "\n",
    "with open(\"../Data_1.txt\",\"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "print(lines)\n",
    "\n",
    "file_line = lines[0]\n",
    "\n",
    "print(file_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e7320",
   "metadata": {},
   "source": [
    "#### 1. Word Tokenization using **Split Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "497d14f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Textual\n",
      "2. information\n",
      "3. in\n",
      "4. the\n",
      "5. world\n",
      "6. can\n",
      "7. be\n",
      "8. broadly\n",
      "9. categorized\n",
      "10. into\n",
      "11. two\n",
      "12. main\n",
      "13. types:\n",
      "14. facts\n",
      "15. and\n",
      "16. opinions.\n",
      "17. Facts\n",
      "18. are\n",
      "19. objective\n",
      "20. expressions\n",
      "21. about\n",
      "22. entities,\n",
      "23. events,\n",
      "24. and\n",
      "25. their\n",
      "26. properties.\n",
      "27. Opinions\n",
      "28. are\n",
      "29. usually\n",
      "30. subjective\n",
      "31. expressions\n",
      "32. that\n",
      "33. describe\n",
      "34. people’s\n",
      "35. sentiments,\n",
      "36. appraisals,\n",
      "37. or\n",
      "38. feelings\n",
      "39. toward\n",
      "40. entities,\n",
      "41. events,\n",
      "42. and\n",
      "43. their\n",
      "44. properties.\n",
      "\n",
      "Total Tokens: 44\n",
      "Total Stop Words: 15\n",
      "Total Punctuations: 0\n"
     ]
    }
   ],
   "source": [
    "split_words = file_line.split(' ')\n",
    "\n",
    "sf_tokens = 0\n",
    "sf_sw = 0\n",
    "sf_puncs = 0\n",
    "\n",
    "for word in split_words:\n",
    "    if DetectStopwords(word):\n",
    "        sf_sw +=1\n",
    "        \n",
    "    if DetectPunctuations(word):\n",
    "        sf_puncs +=1\n",
    "    \n",
    "    print(f'{sf_tokens+1}. {word}')\n",
    "    sf_tokens+=1\n",
    "    \n",
    "print()\n",
    "PrintDetails(sf_tokens,sf_sw,sf_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf1c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "1. Textual\n",
      "2. information\n",
      "3. world\n",
      "4. broadly\n",
      "5. categorized\n",
      "6. two\n",
      "7. main\n",
      "8. types:\n",
      "9. facts\n",
      "10. opinions.\n",
      "11. Facts\n",
      "12. objective\n",
      "13. expressions\n",
      "14. entities,\n",
      "15. events,\n",
      "16. properties.\n",
      "17. Opinions\n",
      "18. usually\n",
      "19. subjective\n",
      "20. expressions\n",
      "21. describe\n",
      "22. people’s\n",
      "23. sentiments,\n",
      "24. appraisals,\n",
      "25. feelings\n",
      "26. toward\n",
      "27. entities,\n",
      "28. events,\n",
      "29. properties.\n",
      "\n",
      "Removed Stopwords:\n",
      "1. in (1)\n",
      "2. the (1)\n",
      "3. can (1)\n",
      "4. be (1)\n",
      "5. into (1)\n",
      "6. and (3)\n",
      "7. are (2)\n",
      "8. about (1)\n",
      "9. their (2)\n",
      "10. that (1)\n",
      "11. or (1)\n",
      "\n",
      "Total Tokens: 29\n",
      "Total Stop Words: 15\n",
      "Total Punctuations: 0\n"
     ]
    }
   ],
   "source": [
    "#code with removing stop words and punctuations\n",
    "\n",
    "split_words = file_line.split(' ')\n",
    "\n",
    "sf_tokens = 0\n",
    "sf_sw = 0\n",
    "sf_puncs = 0\n",
    "\n",
    "sw_list = []\n",
    "\n",
    "for word in split_words:\n",
    "    if DetectStopwords(word):\n",
    "        sf_sw +=1\n",
    "        sw_list.append(word)\n",
    "        continue\n",
    "        \n",
    "    if DetectPunctuations(word):\n",
    "        sf_puncs +=1\n",
    "        continue\n",
    "    \n",
    "    if word == split_words[0]:\n",
    "        print(\"Tokens: \")\n",
    "    \n",
    "    print(f'{sf_tokens+1}. {word}')\n",
    "    sf_tokens+=1\n",
    "    \n",
    "print()\n",
    "PrintRemovedSW(sw_list)\n",
    "print()\n",
    "PrintDetails(sf_tokens,sf_sw,sf_puncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553d243",
   "metadata": {},
   "source": [
    "#### 2. Word Tokenization using **Regular Expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14ebb764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.\n"
     ]
    }
   ],
   "source": [
    "print(file_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390d0fc",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "1. ( ) \n",
    "- \n",
    "\n",
    "2. ?:  \n",
    "- non-capturing group is a way to group part of a pattern without capturing the matched text. \n",
    "- Non-capturing groups are useful when you want to ***apply a quantifier*** to part of your regular expression, but you ***don't want*** to ***extract or capture*** that part of the matched text. \n",
    "- eg refer to example below (people's)\n",
    "- ? can be replace to other quantifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc8289b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Textual\n",
      "2. information\n",
      "3. the\n",
      "4. world\n",
      "5. can\n",
      "6. broadly\n",
      "7. categorized\n",
      "8. into\n",
      "9. two\n",
      "10. main\n",
      "11. types\n",
      "12. facts\n",
      "13. and\n",
      "14. opinions\n",
      "15. Facts\n",
      "16. are\n",
      "17. objective\n",
      "18. expressions\n",
      "19. about\n",
      "20. entities\n",
      "21. events\n",
      "22. and\n",
      "23. their\n",
      "24. properties\n",
      "25. Opinions\n",
      "26. are\n",
      "27. usually\n",
      "28. subjective\n",
      "29. expressions\n",
      "30. that\n",
      "31. describe\n",
      "32. people’s\n",
      "33. sentiments\n",
      "34. appraisals\n",
      "35. feelings\n",
      "36. toward\n",
      "37. entities\n",
      "38. events\n",
      "39. and\n",
      "40. their\n",
      "41. properties\n",
      "\n",
      "Total Tokens: 41\n",
      "Total Stop Words: 12\n",
      "Total Punctuations: 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\b\\w+(?:.\\w)\\b\"\n",
    "\n",
    "tokens = re.findall(pattern, file_line)\n",
    "\n",
    "sw_list = []\n",
    "\n",
    "re_tokens = 0\n",
    "re_sw = 0\n",
    "re_puncs = 0\n",
    "\n",
    "for token in tokens:\n",
    "    if DetectStopwords(token):\n",
    "        re_sw +=1\n",
    "        \n",
    "    if DetectPunctuations(token):\n",
    "        re_puncs +=1\n",
    "    \n",
    "    print(f'{re_tokens+1}. {token}')\n",
    "    re_tokens+=1\n",
    "    \n",
    "print()\n",
    "PrintDetails(re_tokens,re_sw,re_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ca539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "1. Textual\n",
      "2. information\n",
      "3. world\n",
      "4. broadly\n",
      "5. categorized\n",
      "6. two\n",
      "7. main\n",
      "8. types\n",
      "9. facts\n",
      "10. opinions\n",
      "11. Facts\n",
      "12. objective\n",
      "13. expressions\n",
      "14. entities\n",
      "15. events\n",
      "16. properties\n",
      "17. Opinions\n",
      "18. usually\n",
      "19. subjective\n",
      "20. expressions\n",
      "21. describe\n",
      "22. people’s\n",
      "23. sentiments\n",
      "24. appraisals\n",
      "25. feelings\n",
      "26. toward\n",
      "27. entities\n",
      "28. events\n",
      "29. properties\n",
      "\n",
      "Removed Stopwords:\n",
      "1. the (1)\n",
      "2. can (1)\n",
      "3. into (1)\n",
      "4. and (3)\n",
      "5. are (2)\n",
      "6. about (1)\n",
      "7. their (2)\n",
      "8. that (1)\n",
      "\n",
      "Total Tokens: 29\n",
      "Total Stop Words: 12\n",
      "Total Punctuations: 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\b\\w+(?:.\\w)\\b\"\n",
    "\n",
    "tokens = re.findall(pattern, file_line)\n",
    "\n",
    "sw_list = []\n",
    "\n",
    "re_tokens = 0\n",
    "re_sw = 0\n",
    "re_puncs = 0\n",
    "\n",
    "for token in tokens:\n",
    "    if DetectStopwords(token):\n",
    "        re_sw +=1\n",
    "        sw_list.append(token)\n",
    "        continue\n",
    "        \n",
    "    if DetectPunctuations(token):\n",
    "        re_puncs +=1\n",
    "        continue\n",
    "    \n",
    "    if token == tokens[0]:\n",
    "        print(\"Tokens: \")\n",
    "    \n",
    "    print(f'{re_tokens+1}. {token}')\n",
    "    re_tokens+=1\n",
    "    \n",
    "print()\n",
    "PrintRemovedSW(sw_list)\n",
    "print()\n",
    "PrintDetails(re_tokens,re_sw,re_puncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9922fa2",
   "metadata": {},
   "source": [
    "##### Capturing & Non-Capturing exmaples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1926fad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Using re.search()-----\n",
      "<built-in method group of re.Match object at 0x000001FC3C8DB240>\n",
      "123\n",
      "\n",
      "-----Using re.findall()-----\n",
      "[('123', '4567')]\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Input text\n",
    "input_text = \"Phone number: 123-4567\"\n",
    "\n",
    "# Regular expression pattern with capturing groups\n",
    "pattern = r\"(\\d{3})-(\\d{4})\"\n",
    "\n",
    "# Use re.search() to find the first match\n",
    "match1 = re.search(pattern, input_text)\n",
    "match2 = re.findall(pattern, input_text)\n",
    "\n",
    "print('-'*5 + 'Using re.search()' + '-'*5)\n",
    "print(match1.group)\n",
    "print(match1.group(1))\n",
    "\n",
    "print('\\n'+'-'*5 + 'Using re.findall()' + '-'*5)\n",
    "print(match2)\n",
    "print(match2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9714516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'hell0']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Input text\n",
    "input_text = \"hello world hell0\"\n",
    "\n",
    "# Regular expression pattern with a capturing group\n",
    "pattern1 = r\"(?:hell)\\w+\"\n",
    "pattern2 = r\"ss(ab)\"\n",
    "pattern3 = r\"(?:ab)\"\n",
    "\n",
    "# Use re.search() to find the first match\n",
    "match1 = re.findall(pattern1, input_text)\n",
    "match2 = re.findall(pattern2, input_text)\n",
    "match3 = re.findall(pattern3, input_text)\n",
    "\n",
    "print(match1)\n",
    "print(match2)\n",
    "print(match3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e8147",
   "metadata": {},
   "source": [
    "#### 3. Word Tokenisation using **NLTK packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d59b943d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.\n"
     ]
    }
   ],
   "source": [
    "print(file_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd9d5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Justin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Some NLTK functions, including word_tokenize, require additional data files to be downloaded. \n",
    "# Hence, use nltk package to download using 'punkt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df3ef495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Textual\n",
      "2. information\n",
      "3. in\n",
      "4. the\n",
      "5. world\n",
      "6. can\n",
      "7. be\n",
      "8. broadly\n",
      "9. categorized\n",
      "10. into\n",
      "11. two\n",
      "12. main\n",
      "13. types\n",
      "14. :\n",
      "15. facts\n",
      "16. and\n",
      "17. opinions\n",
      "18. .\n",
      "19. Facts\n",
      "20. are\n",
      "21. objective\n",
      "22. expressions\n",
      "23. about\n",
      "24. entities\n",
      "25. ,\n",
      "26. events\n",
      "27. ,\n",
      "28. and\n",
      "29. their\n",
      "30. properties\n",
      "31. .\n",
      "32. Opinions\n",
      "33. are\n",
      "34. usually\n",
      "35. subjective\n",
      "36. expressions\n",
      "37. that\n",
      "38. describe\n",
      "39. people\n",
      "40. ’\n",
      "41. s\n",
      "42. sentiments\n",
      "43. ,\n",
      "44. appraisals\n",
      "45. ,\n",
      "46. or\n",
      "47. feelings\n",
      "48. toward\n",
      "49. entities\n",
      "50. ,\n",
      "51. events\n",
      "52. ,\n",
      "53. and\n",
      "54. their\n",
      "55. properties\n",
      "56. .\n",
      "\n",
      "Total Tokens: 56\n",
      "Total Stop Words: 16\n",
      "Total Punctuations: 16\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(file_line)\n",
    "\n",
    "nltk_tokens = 0\n",
    "nltk_sw = 0\n",
    "nltk_puncs = 0\n",
    "\n",
    "for token in tokens:\n",
    "    if DetectStopwords(token):\n",
    "        nltk_sw +=1\n",
    "        \n",
    "    if DetectStopwords(token):\n",
    "        nltk_puncs +=1\n",
    "    \n",
    "    print(f'{nltk_tokens+1}. {token}')\n",
    "    nltk_tokens+=1\n",
    "    \n",
    "print()\n",
    "PrintDetails(nltk_tokens,nltk_sw,nltk_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cef5b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "1. Textual\n",
      "2. information\n",
      "3. world\n",
      "4. broadly\n",
      "5. categorized\n",
      "6. two\n",
      "7. main\n",
      "8. types\n",
      "9. facts\n",
      "10. opinions\n",
      "11. Facts\n",
      "12. objective\n",
      "13. expressions\n",
      "14. entities\n",
      "15. events\n",
      "16. properties\n",
      "17. Opinions\n",
      "18. usually\n",
      "19. subjective\n",
      "20. expressions\n",
      "21. describe\n",
      "22. people\n",
      "23. sentiments\n",
      "24. appraisals\n",
      "25. feelings\n",
      "26. toward\n",
      "27. entities\n",
      "28. events\n",
      "29. properties\n",
      "\n",
      "Removed Stopwords:\n",
      "1. in (1)\n",
      "2. the (1)\n",
      "3. can (1)\n",
      "4. be (1)\n",
      "5. into (1)\n",
      "6. and (3)\n",
      "7. are (2)\n",
      "8. about (1)\n",
      "9. their (2)\n",
      "10. that (1)\n",
      "11. s (1)\n",
      "12. or (1)\n",
      "\n",
      "Total Tokens: 29\n",
      "Total Stop Words: 16\n",
      "Total Punctuations: 11\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(file_line)\n",
    "\n",
    "nltk_tokens = 0\n",
    "nltk_sw = 0\n",
    "nltk_puncs = 0\n",
    "\n",
    "sw_list = []\n",
    "\n",
    "for token in tokens:\n",
    "    if DetectStopwords(token):\n",
    "        nltk_sw +=1\n",
    "        sw_list.append(token)\n",
    "        continue\n",
    "        \n",
    "    if DetectPunctuations(token):\n",
    "        nltk_puncs +=1\n",
    "        continue\n",
    "    \n",
    "    if token == tokens[0]:\n",
    "        print(\"Tokens: \")\n",
    "    \n",
    "    print(f'{nltk_tokens+1}. {token}')\n",
    "    nltk_tokens+=1\n",
    "    \n",
    "print()\n",
    "PrintRemovedSW(sw_list)\n",
    "print()\n",
    "PrintDetails(nltk_tokens,nltk_sw,nltk_puncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67c5ce",
   "metadata": {},
   "source": [
    "## Q2 - Form word stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711bc37d",
   "metadata": {},
   "source": [
    "##### 1. Word Stemming using **Regular Expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "506e07d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.\n"
     ]
    }
   ],
   "source": [
    "print(file_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem = re.findall(regexp,word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee2b4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Textual\n",
      "2. information\n",
      "3. in\n",
      "4. the\n",
      "5. world\n",
      "6. can\n",
      "7. be\n",
      "8. broadly\n",
      "9. categorized\n",
      "10. into\n",
      "11. two\n",
      "12. main\n",
      "13. types\n",
      "14. facts\n",
      "15. and\n",
      "16. opinions\n",
      "17. Facts\n",
      "18. are\n",
      "19. objective\n",
      "20. expressions\n",
      "21. about\n",
      "22. entities\n",
      "23. events\n",
      "24. and\n",
      "25. their\n",
      "26. properties\n",
      "27. Opinions\n",
      "28. are\n",
      "29. usually\n",
      "30. subjective\n",
      "31. expressions\n",
      "32. that\n",
      "33. describe\n",
      "34. people\n",
      "35. sentiments\n",
      "36. appraisals\n",
      "37. or\n",
      "38. feelings\n",
      "39. toward\n",
      "40. entities\n",
      "41. events\n",
      "42. and\n",
      "43. their\n",
      "44. properties\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(file_line)\n",
    "\n",
    "count = 0 \n",
    "for token in tokens:\n",
    "    if DetectPunctuations(token):\n",
    "        continue\n",
    "    \n",
    "    if len(token) == 1 and DetectStopwords(token):\n",
    "        continue\n",
    "    \n",
    "    print(f'{count+1}. {stem(token)[0]}')\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ffeb40",
   "metadata": {},
   "source": [
    "##### 2. Word Stemming using **Porter Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47cbf158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.']\n",
      "textual\n",
      "inform\n",
      "in\n",
      "the\n",
      "world\n",
      "can\n",
      "be\n",
      "broadli\n",
      "categor\n",
      "into\n",
      "two\n",
      "main\n",
      "types:\n",
      "fact\n",
      "and\n",
      "opinions.\n",
      "fact\n",
      "are\n",
      "object\n",
      "express\n",
      "about\n",
      "entities,\n",
      "events,\n",
      "and\n",
      "their\n",
      "properties.\n",
      "opinion\n",
      "are\n",
      "usual\n",
      "subject\n",
      "express\n",
      "that\n",
      "describ\n",
      "people’\n",
      "sentiments,\n",
      "appraisals,\n",
      "or\n",
      "feel\n",
      "toward\n",
      "entities,\n",
      "events,\n",
      "and\n",
      "their\n",
      "properties.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "with open(\"../Data_1.txt\", \"r\") as file:\n",
    "    txt = file.read()\n",
    "\n",
    "word = txt.lower().split()\n",
    "print(word)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "for t in word:\n",
    "    print(porter.stem(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb779b",
   "metadata": {},
   "source": [
    "##### 3. Word Stemming using **Lancaster Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a9ba6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. textual\n",
      "2. inform\n",
      "3. in\n",
      "4. the\n",
      "5. world\n",
      "6. can\n",
      "7. be\n",
      "8. broadli\n",
      "9. categor\n",
      "10. into\n",
      "11. two\n",
      "12. main\n",
      "13. type\n",
      "14. :\n",
      "15. fact\n",
      "16. and\n",
      "17. opinion\n",
      "18. .\n",
      "19. fact\n",
      "20. are\n",
      "21. object\n",
      "22. express\n",
      "23. about\n",
      "24. entiti\n",
      "25. ,\n",
      "26. event\n",
      "27. ,\n",
      "28. and\n",
      "29. their\n",
      "30. properti\n",
      "31. .\n",
      "32. opinion\n",
      "33. are\n",
      "34. usual\n",
      "35. subject\n",
      "36. express\n",
      "37. that\n",
      "38. describ\n",
      "39. peopl\n",
      "40. ’\n",
      "41. s\n",
      "42. sentiment\n",
      "43. ,\n",
      "44. apprais\n",
      "45. ,\n",
      "46. or\n",
      "47. feel\n",
      "48. toward\n",
      "49. entiti\n",
      "50. ,\n",
      "51. event\n",
      "52. ,\n",
      "53. and\n",
      "54. their\n",
      "55. properti\n",
      "56. .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "with open(\"../Data_1.txt\", \"r\") as file:\n",
    "    txt = file.read()\n",
    "\n",
    "word = word_tokenize(txt)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "counter = 1  # Initialize a counter variable\n",
    "\n",
    "for t in word:\n",
    "    stemmed_word = porter.stem(t)\n",
    "    print(f\"{counter}. {stemmed_word}\")\n",
    "    counter += 1  # Increment the counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d31c37",
   "metadata": {},
   "source": [
    "## Q3 - Form Parts of Speech (POS) taggers & Syntactic Analysers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4e0a5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The big black dog barked at the white cat and chased away.']\n",
      "The big black dog barked at the white cat and chased away.\n"
     ]
    }
   ],
   "source": [
    "# Read Data_2.txt\n",
    "\n",
    "with open(\"../Data_2.txt\",\"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "print(lines)\n",
    "\n",
    "file_line_2 = lines[0]\n",
    "\n",
    "print(file_line_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0b754",
   "metadata": {},
   "source": [
    "##### 1.1 POS tagging using **NLTK POS tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "853926c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d600311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = word_tokenize(file_line_2)\n",
    "\n",
    "nltk_pos_tg = nltk.pos_tag(tokenized_text)\n",
    "print(nltk_pos_tg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8f251",
   "metadata": {},
   "source": [
    "##### 1.2 POS tagging using **TextBlob POS tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f800e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TextBlob POS Tagger:\n",
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text\n",
    "with open(\"../Data_2.txt\") as file:\n",
    "    txt = file.read()\n",
    "    \n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(txt)\n",
    "\n",
    "# Perform POS tagging using TextBlob\n",
    "pos_tags = blob.tags\n",
    "\n",
    "print(\"\\nTextBlob POS Tagger:\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b8974",
   "metadata": {},
   "source": [
    "##### 1.3 POS tagging using **Regular Expression tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf6e9ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegexpTagger Output:\n",
      "Word: The \tPOS Tag: NN\n",
      "Word: big \tPOS Tag: NN\n",
      "Word: black \tPOS Tag: NN\n",
      "Word: dog \tPOS Tag: NN\n",
      "Word: barked \tPOS Tag: VBD\n",
      "Word: at \tPOS Tag: NN\n",
      "Word: the \tPOS Tag: NN\n",
      "Word: white \tPOS Tag: NN\n",
      "Word: cat \tPOS Tag: NN\n",
      "Word: and \tPOS Tag: NN\n",
      "Word: chased \tPOS Tag: VBD\n",
      "Word: away \tPOS Tag: NN\n",
      "Word: . \tPOS Tag: NN\n"
     ]
    }
   ],
   "source": [
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),               # gerunds\n",
    "    (r'.*ed$', 'VBD'),                # simple past\n",
    "    (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),               # modals\n",
    "    (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                 # plural nouns\n",
    "    (r'^-?[0-9]+(.[0.9]+)?$', 'CD'),  # cardinal numbers\n",
    "    (r'.*', 'NN'),                    # nouns (default)\n",
    "    (r'^\\d+$', 'CD'),\n",
    "    (r'.*ing$', 'VBG'),               # gerunds, i.e. wondering\n",
    "    (r'.*ment$', 'NN'),               # i.e. wonderment\n",
    "    (r'.*ful$', 'JJ')                 # i.e. wonderful\n",
    "]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "tagger = nltk.tag.sequential.RegexpTagger(patterns)\n",
    "\n",
    "with open(\"../Data_2.txt\", \"r\") as file:\n",
    "    txt = file.read()\n",
    "\n",
    "text1 = word_tokenize(txt)\n",
    "tagged_text = tagger.tag(text1)\n",
    "\n",
    "print(\"RegexpTagger Output:\")\n",
    "for word, tag in tagged_text:\n",
    "    print(f\"Word: {word} \\tPOS Tag: {tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ce450",
   "metadata": {},
   "source": [
    "##### 3. **Parse Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be598ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'big', 'black', 'dog', 'barked', 'at', 'the', 'white', 'cat', 'and', 'chased', 'away']\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"360px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,560.0,360.0\" width=\"560px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"31.4286%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"22.7273%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.3636%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"77.2727%\" x=\"22.7273%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOM</text></svg><svg width=\"29.4118%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">big</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"41.1765%\" x=\"29.4118%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">black</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"29.4118%\" x=\"70.5882%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.2941%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.3636%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.7143%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"68.5714%\" x=\"31.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"60.4167%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"27.5862%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">barked</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.7931%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"72.4138%\" x=\"27.5862%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"19.0476%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.52381%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"80.9524%\" x=\"19.0476%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"29.4118%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"70.5882%\" x=\"29.4118%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOM</text></svg><svg width=\"58.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">white</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.1667%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"41.6667%\" x=\"58.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">cat</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.1667%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.7059%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.5238%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.7931%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.2083%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.4167%\" x=\"60.4167%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"29.1667%\" x=\"70.8333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"57.1429%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">chased</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.5714%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"42.8571%\" x=\"57.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">away</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.5714%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.4167%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.7143%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('NP', [Tree('DT', ['The']), Tree('NOM', [Tree('ADJ', ['big']), Tree('ADJ', ['black']), Tree('NN', ['dog'])])]), Tree('VP', [Tree('VP', [Tree('VB', ['barked']), Tree('PP', [Tree('IN', ['at']), Tree('NP', [Tree('DT', ['the']), Tree('NOM', [Tree('ADJ', ['white']), Tree('NN', ['cat'])])])])]), Tree('CC', ['and']), Tree('VP', [Tree('VB', ['chased']), Tree('RB', ['away'])])])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text2 = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> DT NOM | DT NN\n",
    "NOM -> ADJ ADJ NN | ADJ NN\n",
    "VP -> VP CC VP | VB PP | VB RB | VB\n",
    "PP -> IN NP\n",
    "CC -> 'and'\n",
    "VB -> 'chased' | 'barked'\n",
    "DT -> 'The' | 'the'\n",
    "ADJ -> 'big' | 'black' | 'white'\n",
    "NN -> 'dog' | 'cat'\n",
    "VBD -> 'barked' | 'chased'\n",
    "IN -> 'at' \n",
    "RB -> 'away'\n",
    "\"\"\")\n",
    "\n",
    "with open('../Data_2.txt') as file:\n",
    "    txt = file.read()\n",
    "\n",
    "text1 = [word for word in word_tokenize(txt) if word.isalnum()]\n",
    "print(text1)\n",
    "parser = nltk.ChartParser(text2)\n",
    "for tree1 in parser.parse(text1):\n",
    "    display(tree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Work on Sentence Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - **Unsmoothed Bigram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "from nltk.util import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Corpus\n",
      "\n",
      "~~~~~~~~~~~~~\n",
      "\n",
      "<s> He read a book </s>\n",
      "\n",
      "<s> I read a different book </s>\n",
      "\n",
      "<s> He read a book by Danielle </s>\n",
      "\n",
      "\n",
      "\n",
      "Calculate sentence probability for the following sentence\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "<s> I read a book by Danielle </s>\n"
     ]
    }
   ],
   "source": [
    "with open(\"../Data_3.txt\",\"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delete_S(text_corpus_list):\n",
    "    new_list = []\n",
    "    for token in text_corpus_list:\n",
    "        if token == '<' or token == 's' or token == '>' or token == '/s':\n",
    "            continue\n",
    "        new_list.append(token)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corp_1 = lines[2]\n",
    "training_corp_2 = lines[3]\n",
    "training_corp_3 = lines[4]\n",
    "\n",
    "test_corp = lines[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 's', '>', 'he', 'read', 'a', 'book', '<', '/s', '>']\n"
     ]
    }
   ],
   "source": [
    "tokens_trc1 = nltk.tokenize.word_tokenize(training_corp_1.lower())\n",
    "tokens_trc2 = nltk.tokenize.word_tokenize(training_corp_2.lower())\n",
    "tokens_trc3 = nltk.tokenize.word_tokenize(training_corp_3.lower())\n",
    "tokens_tec = nltk.tokenize.word_tokenize(test_corp.lower())\n",
    "\n",
    "print(tokens_trc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'read', 'a', 'book']\n",
      "['i', 'read', 'a', 'different', 'book']\n",
      "['he', 'read', 'a', 'book', 'by', 'danielle']\n",
      "['i', 'read', 'a', 'book', 'by', 'danielle']\n"
     ]
    }
   ],
   "source": [
    "# tokens of training corpus\n",
    "tokens_trc1 = Delete_S(tokens_trc1)\n",
    "tokens_trc2 = Delete_S(tokens_trc2)\n",
    "tokens_trc3 = Delete_S(tokens_trc3)\n",
    "\n",
    "#tokens of test corpus\n",
    "tokens_tec = Delete_S(tokens_tec)\n",
    "\n",
    "print(tokens_trc1)\n",
    "print(tokens_trc2)\n",
    "print(tokens_trc3)\n",
    "print(tokens_tec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'he'), ('he', 'read'), ('read', 'a'), ('a', 'book'), ('book', '</s>')]\n",
      "[('<s>', 'i'), ('i', 'read'), ('read', 'a'), ('a', 'different'), ('different', 'book'), ('book', '</s>')]\n",
      "[('<s>', 'he'), ('he', 'read'), ('read', 'a'), ('a', 'book'), ('book', 'by'), ('by', 'danielle'), ('danielle', '</s>')]\n",
      "[('<s>', 'i'), ('i', 'read'), ('read', 'a'), ('a', 'book'), ('book', 'by'), ('by', 'danielle'), ('danielle', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "# add start and end of text corpus in each sentence\n",
    "\n",
    "padded_trc1 = list(bigrams(pad_both_ends(tokens_trc1, n=2)))\n",
    "padded_trc2 = list(bigrams(pad_both_ends(tokens_trc2, n=2)))\n",
    "padded_trc3 = list(bigrams(pad_both_ends(tokens_trc3, n=2)))\n",
    "padded_tec = list(bigrams(pad_both_ends(tokens_tec, n=2)))\n",
    "\n",
    "print(padded_trc1)\n",
    "print(padded_trc2)\n",
    "print(padded_trc3)\n",
    "print(padded_tec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('<s>', 'he'),\n",
       "  ('he', 'read'),\n",
       "  ('read', 'a'),\n",
       "  ('a', 'book'),\n",
       "  ('book', '</s>')],\n",
       " [('<s>', 'i'),\n",
       "  ('i', 'read'),\n",
       "  ('read', 'a'),\n",
       "  ('a', 'different'),\n",
       "  ('different', 'book'),\n",
       "  ('book', '</s>')],\n",
       " [('<s>', 'he'),\n",
       "  ('he', 'read'),\n",
       "  ('read', 'a'),\n",
       "  ('a', 'book'),\n",
       "  ('book', 'by'),\n",
       "  ('by', 'danielle'),\n",
       "  ('danielle', '</s>')]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus_combined = []\n",
    "\n",
    "training_corpus_combined.append(padded_trc1)\n",
    "training_corpus_combined.append(padded_trc2)\n",
    "training_corpus_combined.append(padded_trc3)\n",
    "\n",
    "training_corpus_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'i'),\n",
       " ('i', 'read'),\n",
       " ('read', 'a'),\n",
       " ('a', 'book'),\n",
       " ('book', 'by'),\n",
       " ('by', 'danielle'),\n",
       " ('danielle', '</s>')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biagram models for smoothed and unsmoothed conditions\n",
    "def un_smoothed_bigram_models(training_corpus_combined,bigram_test_corpus,is_smoothed):\n",
    "    \n",
    "    probabilty_lst = []\n",
    "    \n",
    "    for bigram_test_corp in bigram_test_corpus:\n",
    "\n",
    "        unique_words = []\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        \n",
    "        for training_corpus in training_corpus_combined:\n",
    "            for bigram_train_corp in training_corpus:\n",
    "                if bigram_train_corp == bigram_test_corp:\n",
    "                    numerator+=1\n",
    "                \n",
    "                if bigram_train_corp[0] == bigram_test_corp[0]:\n",
    "                    denominator+=1\n",
    "                    \n",
    "                if bigram_train_corp[0] not in unique_words:\n",
    "                    unique_words.append(bigram_train_corp[0])\n",
    "                \n",
    "                if bigram_train_corp[1] not in unique_words:\n",
    "                    unique_words.append(bigram_train_corp[1])\n",
    "        \n",
    "        probabilty = numerator/denominator if is_smoothed == False else (numerator)/((denominator)+len(unique_words)) #include <s> and </s>\n",
    "        \n",
    "        probabilty_lst.append(probabilty)\n",
    "        \n",
    "    prob_multiplication = 0\n",
    "        \n",
    "    for probabilty in probabilty_lst:\n",
    "        \n",
    "        if prob_multiplication == 0:\n",
    "            prob_multiplication+=probabilty\n",
    "            continue\n",
    "        \n",
    "        prob_multiplication*=probabilty\n",
    "        \n",
    "    return prob_multiplication\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07407407407407407"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_smoothed_bigram_models(training_corpus_combined,padded_tec,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5783371749621749e-07"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_smoothed_bigram_models(training_corpus_combined,padded_tec,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (txsa_env)",
   "language": "python",
   "name": "txsa_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
